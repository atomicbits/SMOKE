{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before you start:\n",
    "\n",
    "We used the DCNv2 code from https://github.com/3846chs/DCNv2 instead of the original code in the `smoke/csrc` folder.\n",
    "But this code only works with torch 1.7 and CUDA 11.1, so that's what we used in the Dockerfile of this project \n",
    "\n",
    "Seems to work: build a docker based on torch 1.7 and CUDA 11.1 ==> we worked with these versions in our Dockerfile.\n",
    "\n",
    "First do:\n",
    "`python setup.py build develop`\n",
    "in a shell at the top-level of the project to compile the C code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from smoke.config import cfg\n",
    "from smoke.engine import (\n",
    "    default_argument_parser,\n",
    "    default_setup\n",
    ")\n",
    "from smoke.utils.check_point import DetectronCheckpointer\n",
    "from smoke.modeling.detector import build_detection_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(args):\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    default_setup(cfg, args)\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_infer(args):\n",
    "    cfg = setup(args)\n",
    "\n",
    "    model = build_detection_model(cfg)\n",
    "    model.training = False\n",
    "    model.backbone.training = False\n",
    "    model.heads.training = False\n",
    "    device = torch.device(cfg.MODEL.DEVICE)\n",
    "    model.to(device)\n",
    "\n",
    "    checkpointer = DetectronCheckpointer(cfg, model, save_dir=cfg.OUTPUT_DIR)\n",
    "    ckpt = cfg.MODEL.WEIGHT if args.ckpt is None else args.ckpt\n",
    "    _ = checkpointer.load(ckpt, use_latest=args.ckpt is None)\n",
    "\n",
    "    # Read a PIL image\n",
    "    image = Image.open('data/kylle-pangan-unsplash.jpg')\n",
    "    image = image.resize([640, 320]) # we probably don't need to make it this small, but the original size gives an OOM \n",
    "    # Define a transform to convert PIL image to a Torch tensor\n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.PILToTensor()\n",
    "    # ])\n",
    "    transform = transforms.PILToTensor()\n",
    "    # Convert the PIL image to Torch tensor\n",
    "    img_tensor = transform(image)\n",
    "\n",
    "    img_tensor = img_tensor.to(device)\n",
    "    output = model(img_tensor.float())\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    output = output.to(cpu_device)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command Line Args: Namespace(ckpt=None, config_file='configs/smoke_gn_vector.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=[])\n",
      "[2022-12-05 08:16:44,927] smoke INFO: Using 1 GPUs\n",
      "[2022-12-05 08:16:44,929] smoke INFO: Collecting environment info\n",
      "[2022-12-05 08:16:46,556] smoke INFO: \n",
      "PyTorch version: 1.7.0\n",
      "Is debug build: True\n",
      "CUDA used to build PyTorch: 11.0\n",
      "ROCM used to build PyTorch: N/A\n",
      "\n",
      "OS: Ubuntu 18.04.5 LTS (x86_64)\n",
      "GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "Clang version: Could not collect\n",
      "CMake version: Could not collect\n",
      "\n",
      "Python version: 3.8 (64-bit runtime)\n",
      "Is CUDA available: True\n",
      "CUDA runtime version: Could not collect\n",
      "GPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\n",
      "Nvidia driver version: 510.85.02\n",
      "cuDNN version: Probably one of the following:\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4\n",
      "/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4\n",
      "HIP runtime version: N/A\n",
      "MIOpen runtime version: N/A\n",
      "\n",
      "Versions of relevant libraries:\n",
      "[pip3] numpy==1.19.2\n",
      "[pip3] torch==1.7.0\n",
      "[pip3] torchelastic==0.2.1\n",
      "[pip3] torchvision==0.8.0\n",
      "[conda] blas                      1.0                         mkl  \n",
      "[conda] cudatoolkit               11.0.221             h6bb024c_0  \n",
      "[conda] mkl                       2020.2                      256  \n",
      "[conda] mkl-service               2.3.0            py38he904b0f_0  \n",
      "[conda] mkl_fft                   1.2.0            py38h23d657b_0  \n",
      "[conda] mkl_random                1.1.1            py38h0573a6f_0  \n",
      "[conda] numpy                     1.19.2           py38h54aff64_0  \n",
      "[conda] numpy-base                1.19.2           py38hfa32c7d_0  \n",
      "[conda] pytorch                   1.7.0           py3.8_cuda11.0.221_cudnn8.0.3_0    pytorch\n",
      "[conda] torchelastic              0.2.1                    pypi_0    pypi\n",
      "[conda] torchvision               0.8.0                py38_cu110    pytorch\n",
      "        Pillow (9.3.0)\n",
      "[2022-12-05 08:16:46,557] smoke INFO: Namespace(ckpt=None, config_file='configs/smoke_gn_vector.yaml', dist_url='tcp://127.0.0.1:50152', eval_only=True, machine_rank=0, num_gpus=1, num_machines=1, opts=[])\n",
      "[2022-12-05 08:16:46,558] smoke INFO: Loaded configuration file configs/smoke_gn_vector.yaml\n",
      "[2022-12-05 08:16:46,559] smoke INFO: \n",
      "MODEL:\n",
      "  WEIGHT: \"catalog://ImageNetPretrained/DLA34\"\n",
      "INPUT:\n",
      "  FLIP_PROB_TRAIN: 0.5\n",
      "  SHIFT_SCALE_PROB_TRAIN: 0.3\n",
      "DATASETS:\n",
      "  DETECT_CLASSES: (\"Car\", \"Cyclist\", \"Pedestrian\")\n",
      "  TRAIN: (\"kitti_train\",)\n",
      "  TEST: (\"kitti_test\",)\n",
      "  TRAIN_SPLIT: \"trainval\"\n",
      "  TEST_SPLIT: \"test\"\n",
      "SOLVER:\n",
      "  BASE_LR: 2.5e-4\n",
      "  STEPS: (10000, 18000)\n",
      "  MAX_ITERATION: 25000\n",
      "  IMS_PER_BATCH: 32\n",
      "[2022-12-05 08:16:46,560] smoke INFO: Running with config:\n",
      "CUDNN_BENCHMARK: True\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: False\n",
      "  NUM_WORKERS: 4\n",
      "  SIZE_DIVISIBILITY: 0\n",
      "DATASETS:\n",
      "  DETECT_CLASSES: ('Car', 'Cyclist', 'Pedestrian')\n",
      "  MAX_OBJECTS: 30\n",
      "  TEST: ('kitti_test',)\n",
      "  TEST_SPLIT: test\n",
      "  TRAIN: ('kitti_train',)\n",
      "  TRAIN_SPLIT: trainval\n",
      "INPUT:\n",
      "  FLIP_PROB_TRAIN: 0.5\n",
      "  HEIGHT_TEST: 384\n",
      "  HEIGHT_TRAIN: 384\n",
      "  PIXEL_MEAN: [0.485, 0.456, 0.406]\n",
      "  PIXEL_STD: [0.229, 0.224, 0.225]\n",
      "  SHIFT_SCALE_PROB_TRAIN: 0.3\n",
      "  SHIFT_SCALE_TRAIN: (0.2, 0.4)\n",
      "  TO_BGR: True\n",
      "  WIDTH_TEST: 1280\n",
      "  WIDTH_TRAIN: 1280\n",
      "MODEL:\n",
      "  BACKBONE:\n",
      "    BACKBONE_OUT_CHANNELS: 64\n",
      "    CONV_BODY: DLA-34-DCN\n",
      "    DOWN_RATIO: 4\n",
      "    FREEZE_CONV_BODY_AT: 0\n",
      "    USE_NORMALIZATION: GN\n",
      "  DEVICE: cuda\n",
      "  GROUP_NORM:\n",
      "    DIM_PER_GP: -1\n",
      "    EPSILON: 1e-05\n",
      "    NUM_GROUPS: 32\n",
      "  SMOKE_HEAD:\n",
      "    DEPTH_REFERENCE: (28.01, 16.32)\n",
      "    DIMENSION_REFERENCE: ((3.88, 1.63, 1.53), (1.78, 1.7, 0.58), (0.88, 1.73, 0.67))\n",
      "    LOSS_ALPHA: 2\n",
      "    LOSS_BETA: 4\n",
      "    LOSS_TYPE: ('FocalLoss', 'DisL1')\n",
      "    LOSS_WEIGHT: (1.0, 10.0)\n",
      "    NUM_CHANNEL: 256\n",
      "    PREDICTOR: SMOKEPredictor\n",
      "    REGRESSION_CHANNEL: (1, 2, 3, 2)\n",
      "    REGRESSION_HEADS: 8\n",
      "    USE_NMS: False\n",
      "    USE_NORMALIZATION: GN\n",
      "  SMOKE_ON: True\n",
      "  WEIGHT: catalog://ImageNetPretrained/DLA34\n",
      "OUTPUT_DIR: ./tools/logs\n",
      "PATHS_CATALOG: /work/smoke/config/paths_catalog.py\n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  BASE_LR: 0.00025\n",
      "  BIAS_LR_FACTOR: 2\n",
      "  CHECKPOINT_PERIOD: 20\n",
      "  EVALUATE_PERIOD: 20\n",
      "  IMS_PER_BATCH: 32\n",
      "  LOAD_OPTIMIZER_SCHEDULER: True\n",
      "  MASTER_BATCH: -1\n",
      "  MAX_ITERATION: 25000\n",
      "  OPTIMIZER: Adam\n",
      "  STEPS: (10000, 18000)\n",
      "TEST:\n",
      "  DETECTIONS_PER_IMG: 50\n",
      "  DETECTIONS_THRESHOLD: 0.25\n",
      "  IMS_PER_BATCH: 1\n",
      "  PRED_2D: True\n",
      "  SINGLE_GPU_TEST: True\n",
      "[2022-12-05 08:16:46,561] smoke.utils.envs INFO: Using a generated random seed 46696367\n",
      "[2022-12-05 08:16:48,549] smoke.utils.check_point INFO: Loading checkpoint from catalog://ImageNetPretrained/DLA34\n",
      "[2022-12-05 08:16:48,551] smoke.utils.check_point INFO: catalog://ImageNetPretrained/DLA34 points to http://dl.yf.io/dla/models/imagenet/dla34-ba72cf86.pth\n",
      "[2022-12-05 08:16:48,552] smoke.utils.check_point INFO: url http://dl.yf.io/dla/models/imagenet/dla34-ba72cf86.pth cached in /home/appuser/.torch/models/dla34-ba72cf86.pth\n",
      "[2022-12-05 08:16:48,591] smoke.utils.model_serialization INFO: backbone.body.base.base_layer.0.weight                                loaded from base_layer.0.weight                 of shape (16, 3, 7, 7)\n",
      "[2022-12-05 08:16:48,592] smoke.utils.model_serialization INFO: backbone.body.base.base_layer.1.bias                                  loaded from base_layer.1.bias                   of shape (16,)\n",
      "[2022-12-05 08:16:48,593] smoke.utils.model_serialization INFO: backbone.body.base.base_layer.1.weight                                loaded from base_layer.1.weight                 of shape (16,)\n",
      "[2022-12-05 08:16:48,593] smoke.utils.model_serialization INFO: backbone.body.base.level0.0.weight                                    loaded from level0.0.weight                     of shape (16, 16, 3, 3)\n",
      "[2022-12-05 08:16:48,593] smoke.utils.model_serialization INFO: backbone.body.base.level0.1.bias                                      loaded from level0.1.bias                       of shape (16,)\n",
      "[2022-12-05 08:16:48,594] smoke.utils.model_serialization INFO: backbone.body.base.level0.1.weight                                    loaded from level0.1.weight                     of shape (16,)\n",
      "[2022-12-05 08:16:48,595] smoke.utils.model_serialization INFO: backbone.body.base.level1.0.weight                                    loaded from level1.0.weight                     of shape (32, 16, 3, 3)\n",
      "[2022-12-05 08:16:48,595] smoke.utils.model_serialization INFO: backbone.body.base.level1.1.bias                                      loaded from level1.1.bias                       of shape (32,)\n",
      "[2022-12-05 08:16:48,595] smoke.utils.model_serialization INFO: backbone.body.base.level1.1.weight                                    loaded from level1.1.weight                     of shape (32,)\n",
      "[2022-12-05 08:16:48,596] smoke.utils.model_serialization INFO: backbone.body.base.level2.project.0.weight                            loaded from level2.project.0.weight             of shape (64, 32, 1, 1)\n",
      "[2022-12-05 08:16:48,596] smoke.utils.model_serialization INFO: backbone.body.base.level2.project.1.bias                              loaded from level2.project.1.bias               of shape (64,)\n",
      "[2022-12-05 08:16:48,597] smoke.utils.model_serialization INFO: backbone.body.base.level2.project.1.weight                            loaded from level2.project.1.weight             of shape (64,)\n",
      "[2022-12-05 08:16:48,598] smoke.utils.model_serialization INFO: backbone.body.base.level2.root.conv.weight                            loaded from level2.root.conv.weight             of shape (64, 128, 1, 1)\n",
      "[2022-12-05 08:16:48,598] smoke.utils.model_serialization INFO: backbone.body.base.level2.tree1.conv1.weight                          loaded from level2.tree1.conv1.weight           of shape (64, 32, 3, 3)\n",
      "[2022-12-05 08:16:48,599] smoke.utils.model_serialization INFO: backbone.body.base.level2.tree1.conv2.weight                          loaded from level2.tree1.conv2.weight           of shape (64, 64, 3, 3)\n",
      "[2022-12-05 08:16:48,599] smoke.utils.model_serialization INFO: backbone.body.base.level2.tree2.conv1.weight                          loaded from level2.tree2.conv1.weight           of shape (64, 64, 3, 3)\n",
      "[2022-12-05 08:16:48,599] smoke.utils.model_serialization INFO: backbone.body.base.level2.tree2.conv2.weight                          loaded from level2.tree2.conv2.weight           of shape (64, 64, 3, 3)\n",
      "[2022-12-05 08:16:48,600] smoke.utils.model_serialization INFO: backbone.body.base.level3.project.0.weight                            loaded from level3.project.0.weight             of shape (128, 64, 1, 1)\n",
      "[2022-12-05 08:16:48,600] smoke.utils.model_serialization INFO: backbone.body.base.level3.project.1.bias                              loaded from level3.project.1.bias               of shape (128,)\n",
      "[2022-12-05 08:16:48,601] smoke.utils.model_serialization INFO: backbone.body.base.level3.project.1.weight                            loaded from level3.project.1.weight             of shape (128,)\n",
      "[2022-12-05 08:16:48,601] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.project.0.weight                      loaded from level3.tree1.project.0.weight       of shape (128, 64, 1, 1)\n",
      "[2022-12-05 08:16:48,602] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.project.1.bias                        loaded from level3.tree1.project.1.bias         of shape (128,)\n",
      "[2022-12-05 08:16:48,602] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.project.1.weight                      loaded from level3.tree1.project.1.weight       of shape (128,)\n",
      "[2022-12-05 08:16:48,603] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.root.conv.weight                      loaded from level3.tree1.root.conv.weight       of shape (128, 256, 1, 1)\n",
      "[2022-12-05 08:16:48,603] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.tree1.conv1.weight                    loaded from level3.tree1.tree1.conv1.weight     of shape (128, 64, 3, 3)\n",
      "[2022-12-05 08:16:48,603] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.tree1.conv2.weight                    loaded from level3.tree1.tree1.conv2.weight     of shape (128, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,605] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.tree2.conv1.weight                    loaded from level3.tree1.tree2.conv1.weight     of shape (128, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,606] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree1.tree2.conv2.weight                    loaded from level3.tree1.tree2.conv2.weight     of shape (128, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,606] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree2.root.conv.weight                      loaded from level3.tree2.root.conv.weight       of shape (128, 448, 1, 1)\n",
      "[2022-12-05 08:16:48,607] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree2.tree1.conv1.weight                    loaded from level3.tree2.tree1.conv1.weight     of shape (128, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,607] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree2.tree1.conv2.weight                    loaded from level3.tree2.tree1.conv2.weight     of shape (128, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,608] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree2.tree2.conv1.weight                    loaded from level3.tree2.tree2.conv1.weight     of shape (128, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,608] smoke.utils.model_serialization INFO: backbone.body.base.level3.tree2.tree2.conv2.weight                    loaded from level3.tree2.tree2.conv2.weight     of shape (128, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,609] smoke.utils.model_serialization INFO: backbone.body.base.level4.project.0.weight                            loaded from level4.project.0.weight             of shape (256, 128, 1, 1)\n",
      "[2022-12-05 08:16:48,609] smoke.utils.model_serialization INFO: backbone.body.base.level4.project.1.bias                              loaded from level4.project.1.bias               of shape (256,)\n",
      "[2022-12-05 08:16:48,610] smoke.utils.model_serialization INFO: backbone.body.base.level4.project.1.weight                            loaded from level4.project.1.weight             of shape (256,)\n",
      "[2022-12-05 08:16:48,610] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.project.0.weight                      loaded from level4.tree1.project.0.weight       of shape (256, 128, 1, 1)\n",
      "[2022-12-05 08:16:48,611] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.project.1.bias                        loaded from level4.tree1.project.1.bias         of shape (256,)\n",
      "[2022-12-05 08:16:48,611] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.project.1.weight                      loaded from level4.tree1.project.1.weight       of shape (256,)\n",
      "[2022-12-05 08:16:48,612] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.root.conv.weight                      loaded from level4.tree1.root.conv.weight       of shape (256, 512, 1, 1)\n",
      "[2022-12-05 08:16:48,613] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.tree1.conv1.weight                    loaded from level4.tree1.tree1.conv1.weight     of shape (256, 128, 3, 3)\n",
      "[2022-12-05 08:16:48,613] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.tree1.conv2.weight                    loaded from level4.tree1.tree1.conv2.weight     of shape (256, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,614] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.tree2.conv1.weight                    loaded from level4.tree1.tree2.conv1.weight     of shape (256, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,614] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree1.tree2.conv2.weight                    loaded from level4.tree1.tree2.conv2.weight     of shape (256, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,615] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree2.root.conv.weight                      loaded from level4.tree2.root.conv.weight       of shape (256, 896, 1, 1)\n",
      "[2022-12-05 08:16:48,615] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree2.tree1.conv1.weight                    loaded from level4.tree2.tree1.conv1.weight     of shape (256, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,615] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree2.tree1.conv2.weight                    loaded from level4.tree2.tree1.conv2.weight     of shape (256, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,616] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree2.tree2.conv1.weight                    loaded from level4.tree2.tree2.conv1.weight     of shape (256, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,616] smoke.utils.model_serialization INFO: backbone.body.base.level4.tree2.tree2.conv2.weight                    loaded from level4.tree2.tree2.conv2.weight     of shape (256, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,617] smoke.utils.model_serialization INFO: backbone.body.base.level5.project.0.weight                            loaded from level5.project.0.weight             of shape (512, 256, 1, 1)\n",
      "[2022-12-05 08:16:48,617] smoke.utils.model_serialization INFO: backbone.body.base.level5.project.1.bias                              loaded from level5.project.1.bias               of shape (512,)\n",
      "[2022-12-05 08:16:48,617] smoke.utils.model_serialization INFO: backbone.body.base.level5.project.1.weight                            loaded from level5.project.1.weight             of shape (512,)\n",
      "[2022-12-05 08:16:48,619] smoke.utils.model_serialization INFO: backbone.body.base.level5.root.conv.weight                            loaded from level5.root.conv.weight             of shape (512, 1280, 1, 1)\n",
      "[2022-12-05 08:16:48,619] smoke.utils.model_serialization INFO: backbone.body.base.level5.tree1.conv1.weight                          loaded from level5.tree1.conv1.weight           of shape (512, 256, 3, 3)\n",
      "[2022-12-05 08:16:48,620] smoke.utils.model_serialization INFO: backbone.body.base.level5.tree1.conv2.weight                          loaded from level5.tree1.conv2.weight           of shape (512, 512, 3, 3)\n",
      "[2022-12-05 08:16:48,620] smoke.utils.model_serialization INFO: backbone.body.base.level5.tree2.conv1.weight                          loaded from level5.tree2.conv1.weight           of shape (512, 512, 3, 3)\n",
      "[2022-12-05 08:16:48,621] smoke.utils.model_serialization INFO: backbone.body.base.level5.tree2.conv2.weight                          loaded from level5.tree2.conv2.weight           of shape (512, 512, 3, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py:127: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.as_tensor(np.asarray(pic))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - - - - - - -\n",
      "projected points, size torch.Size([50, 2]):\n",
      "tensor([[ 41.0000,  74.2562],\n",
      "        [123.0000,  72.7688],\n",
      "        [ 98.0000,  75.6125],\n",
      "        [ 76.0000,  73.4750],\n",
      "        [ 88.0000,  71.5500],\n",
      "        [101.0000,  74.6312],\n",
      "        [ 96.0000,  78.6000],\n",
      "        [ 62.0000,  73.3875],\n",
      "        [ 69.0000,  20.4312],\n",
      "        [135.0000,   3.8438],\n",
      "        [ 66.0000,  19.4125],\n",
      "        [ 55.0000,  49.3438],\n",
      "        [133.0000,   4.8313],\n",
      "        [  4.0000,  59.0250],\n",
      "        [153.0000,   3.9562],\n",
      "        [ 95.0000,  75.5938],\n",
      "        [  5.0000,   4.0312],\n",
      "        [ 69.0000,  25.4312],\n",
      "        [  8.0000,   4.0500],\n",
      "        [ 95.0000,  57.5938],\n",
      "        [125.0000,  74.7812],\n",
      "        [ 57.0000,  49.3563],\n",
      "        [ 70.0000,  18.4375],\n",
      "        [ 70.0000,  23.4375],\n",
      "        [ 50.0000,  60.3125],\n",
      "        [ 87.0000,  78.5438],\n",
      "        [138.0000,   5.8625],\n",
      "        [ 19.0000,   2.1188],\n",
      "        [108.0000,  76.6750],\n",
      "        [122.0000,   4.7625],\n",
      "        [ 76.0000,  78.4750],\n",
      "        [ 35.0000,  77.2188],\n",
      "        [ 59.0000,  41.3688],\n",
      "        [  3.0000,  66.0188],\n",
      "        [108.0000,   4.6750],\n",
      "        [ 72.0000,  73.4500],\n",
      "        [ 10.0000,   4.0625],\n",
      "        [ 86.0000,  46.5375],\n",
      "        [ 54.0000,  61.3375],\n",
      "        [148.0000,   3.9250],\n",
      "        [117.0000,   4.7313],\n",
      "        [ 55.0000,  45.3438],\n",
      "        [ 48.0000,  78.3000],\n",
      "        [ 11.0000,   9.0688],\n",
      "        [  3.0000,  61.0187],\n",
      "        [ 61.0000,  25.3813],\n",
      "        [144.0000,   3.9000],\n",
      "        [ 50.0000,  78.3125],\n",
      "        [158.0000,   0.9875],\n",
      "        [112.0000,  33.7000]], device='cuda:0')\n",
      "- - - - - - - - -\n",
      "point offsets, size torch.Size([50, 2]): \n",
      "tensor([[-0.2914, -0.0889],\n",
      "        [-0.3789, -0.7079],\n",
      "        [-0.9505, -0.5751],\n",
      "        [-0.5989, -0.6920],\n",
      "        [-0.8556, -0.7732],\n",
      "        [-0.7407, -0.5599],\n",
      "        [-0.9948, -0.3248],\n",
      "        [-0.7798, -0.4927],\n",
      "        [-0.4881, -0.8964],\n",
      "        [-0.6033, -0.4365],\n",
      "        [-0.2088, -0.7431],\n",
      "        [-0.5037, -0.8797],\n",
      "        [-0.7344, -0.4983],\n",
      "        [-0.7989, -0.5182],\n",
      "        [-0.7266, -0.5259],\n",
      "        [-0.9935, -0.4338],\n",
      "        [-0.4557, -0.3340],\n",
      "        [-0.4958, -1.0158],\n",
      "        [-0.4363, -0.7325],\n",
      "        [-0.2779, -0.6428],\n",
      "        [-0.3332, -0.2841],\n",
      "        [-0.8523, -0.6560],\n",
      "        [-0.1663, -0.4163],\n",
      "        [-0.4406, -0.6791],\n",
      "        [-0.5897, -0.1578],\n",
      "        [-0.9382, -0.5114],\n",
      "        [-0.9947, -0.6509],\n",
      "        [-0.7598, -0.9021],\n",
      "        [-0.9106, -0.1428],\n",
      "        [-0.5624, -0.6597],\n",
      "        [-0.9311, -0.4465],\n",
      "        [-0.9480, -0.1123],\n",
      "        [-0.0851, -0.3771],\n",
      "        [-0.7917, -0.1605],\n",
      "        [-0.6312, -0.3019],\n",
      "        [-0.7365, -0.6959],\n",
      "        [-0.4554, -0.5086],\n",
      "        [-0.4610, -0.3694],\n",
      "        [-0.2923, -0.4170],\n",
      "        [-0.3356, -0.3019],\n",
      "        [-0.7413, -0.5838],\n",
      "        [-0.4525, -0.4232],\n",
      "        [-0.9140, -0.1938],\n",
      "        [-0.9790, -0.6309],\n",
      "        [-1.2797, -0.3674],\n",
      "        [-0.6151, -0.8960],\n",
      "        [-0.6638, -0.3975],\n",
      "        [-0.9475, -0.3374],\n",
      "        [-0.6947, -0.2082],\n",
      "        [-0.5372, -0.3607]], device='cuda:0', grad_fn=<SliceBackward>)\n",
      "- - - - - - - - -\n",
      "scores, size torch.Size([1, 50]): \n",
      "tensor([[0.1804, 0.1735, 0.1730, 0.1718, 0.1670, 0.1652, 0.1632, 0.1628, 0.1593,\n",
      "         0.1575, 0.1566, 0.1530, 0.1493, 0.1493, 0.1485, 0.1483, 0.1480, 0.1479,\n",
      "         0.1472, 0.1469, 0.1467, 0.1454, 0.1454, 0.1444, 0.1427, 0.1421, 0.1420,\n",
      "         0.1416, 0.1405, 0.1402, 0.1395, 0.1392, 0.1384, 0.1379, 0.1375, 0.1372,\n",
      "         0.1372, 0.1371, 0.1368, 0.1365, 0.1363, 0.1362, 0.1361, 0.1359, 0.1356,\n",
      "         0.1355, 0.1348, 0.1348, 0.1346, 0.1342]], device='cuda:0',\n",
      "       grad_fn=<TopkBackward>)\n",
      "- - - - - - - - -\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m default_argument_parser()\u001b[39m.\u001b[39mparse_args([\u001b[39m'\u001b[39m\u001b[39m--eval-only\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m--config-file\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mconfigs/smoke_gn_vector.yaml\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCommand Line Args:\u001b[39m\u001b[39m\"\u001b[39m, args)\n\u001b[0;32m----> 3\u001b[0m output \u001b[39m=\u001b[39m do_infer(args)\n\u001b[1;32m      5\u001b[0m output\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mdo_infer\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     24\u001b[0m img_tensor \u001b[39m=\u001b[39m transform(image)\n\u001b[1;32m     26\u001b[0m img_tensor \u001b[39m=\u001b[39m img_tensor\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 27\u001b[0m output \u001b[39m=\u001b[39m model(img_tensor\u001b[39m.\u001b[39;49mfloat())\n\u001b[1;32m     28\u001b[0m cpu_device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto(cpu_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/work/smoke/modeling/detector/keypoint_detector.py:37\u001b[0m, in \u001b[0;36mKeypointDetector.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     35\u001b[0m images \u001b[39m=\u001b[39m to_image_list(images)\n\u001b[1;32m     36\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackbone(images\u001b[39m.\u001b[39mtensors)\n\u001b[0;32m---> 37\u001b[0m result, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mheads(features, targets)\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m     40\u001b[0m     losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/work/smoke/modeling/heads/smoke_head/smoke_head.py:27\u001b[0m, in \u001b[0;36mSMOKEHead.forward\u001b[0;34m(self, features, targets)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m {}, \u001b[39mdict\u001b[39m(hm_loss\u001b[39m=\u001b[39mloss_heatmap,\n\u001b[1;32m     25\u001b[0m                     reg_loss\u001b[39m=\u001b[39mloss_regression, )\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m---> 27\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpost_processor(x, targets)\n\u001b[1;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m result, {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/work/smoke/modeling/heads/smoke_head/inference.py:81\u001b[0m, in \u001b[0;36mPostProcessor.forward\u001b[0;34m(self, predictions, targets)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m- - - - - - - - -\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m pred_depths \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmoke_coder\u001b[39m.\u001b[39mdecode_depth(pred_depths_offset)\n\u001b[0;32m---> 81\u001b[0m pred_locations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msmoke_coder\u001b[39m.\u001b[39;49mdecode_location(\n\u001b[1;32m     82\u001b[0m     pred_proj_points,\n\u001b[1;32m     83\u001b[0m     pred_proj_offsets,\n\u001b[1;32m     84\u001b[0m     pred_depths,\n\u001b[1;32m     85\u001b[0m     target_varibales[\u001b[39m\"\u001b[39;49m\u001b[39mK\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     86\u001b[0m     target_varibales[\u001b[39m\"\u001b[39;49m\u001b[39mtrans_mat\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m pred_dimensions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmoke_coder\u001b[39m.\u001b[39mdecode_dimension(\n\u001b[1;32m     89\u001b[0m     clses,\n\u001b[1;32m     90\u001b[0m     pred_dimensions_offsets\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     92\u001b[0m \u001b[39m# we need to change center location to bottom location\u001b[39;00m\n",
      "File \u001b[0;32m/work/smoke/modeling/smoke_coder.py:175\u001b[0m, in \u001b[0;36mSMOKECoder.decode_location\u001b[0;34m(self, points, points_offset, depths, Ks, trans_mats)\u001b[0m\n\u001b[1;32m    173\u001b[0m proj_points_extend \u001b[39m=\u001b[39m proj_points_extend\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[39m# transform project points back on image\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m proj_points_img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(trans_mats_inv, proj_points_extend)\n\u001b[1;32m    176\u001b[0m \u001b[39m# with depth\u001b[39;00m\n\u001b[1;32m    177\u001b[0m proj_points_img \u001b[39m=\u001b[39m proj_points_img \u001b[39m*\u001b[39m depths\u001b[39m.\u001b[39mview(N, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "args = default_argument_parser().parse_args(['--eval-only', '--config-file', 'configs/smoke_gn_vector.yaml'])\n",
    "print(\"Command Line Args:\", args)\n",
    "output = do_infer(args)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 9), grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(profile=\"full\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
